# Prompt Learning `Beta`

![](https://img.shields.io/github/last-commit/WebGao/Prompt-Learning?color=green) ![](https://img.shields.io/badge/PaperNumber-14-brightgreen) ![](https://img.shields.io/badge/PRs-Welcome-red) 

## Contents
- [Basics of Prompt](#basics-of-prompt)
- [Prompt @ Recommendation](#prompt--recommendation)



# Basics of Prompt

### 1. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference (EACL 2021) [[pdf](./Paper/Prompt/Exploiting_Clone/Exploiting%20Cloze%20Questions%20for%20Few%20Shot%20Text%20Classification%20and%20Natural%20Language%20Inference%20-%20EACL%202021.pdf)]

`hard prompt` `MLMå‚æ•°æ›´æ–°`

> ### Motivation
> - ç”±äºè¯­è¨€ã€é¢†åŸŸå’Œä»»åŠ¡çš„æ ‡æ³¨æˆæœ¬æé«˜ï¼Œåœ¨å®é™…å°†è¯­è¨€æ¨¡å‹åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œlabeled sampleså¾ˆçè´µï¼Œfew-shotåœºæ™¯éå¸¸å¸¸è§ï¼Œé™åˆ¶æœ‰ç›‘ç£å­¦ä¹ ï¼Œå› è€Œå¼•èµ·å­¦è€…ã€ä»ä¸šè€…ä»¬å¯¹few-shot settingä¸‹æ‰§è¡ŒNLPä»»åŠ¡çš„å…³æ³¨ã€‚
> - ç»™é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¼•å…¥æ–‡æœ¬è§£é‡Š/ä»»åŠ¡æè¿°ï¼Œå¯ä»¥é€šè¿‡æ— ç›‘ç£æ–¹å¼æœ‰æ•ˆè§£å†³ä¸€äº›NLPä»»åŠ¡ï¼ˆzero-shot scenarioï¼‰ã€‚
> - ç»¼åˆè€ƒè™‘ï¼Œæœ¬æ–‡æå‡ºå¼•å…¥pattern-exploiting-trainingï¼Œè®¾è®¡ä¸€ç§åŠç›‘ç£è®­ç»ƒç­–ç•¥PETå’Œå…¶æ”¹è¿›ç‰ˆæœ¬iPETï¼Œå°†NLPçš„è¾“å…¥samplesï¼ˆtextï¼‰æ˜ å°„ä¸ºå¡«ç©ºé¢˜å½¢å¼ï¼ˆcloze-style phraseï¼‰ã€‚

<details>
<summary>Solution</summary>

> ![Framework of PET](./Paper/Prompt/Exploiting_Clone/fig1.png)

</details>

### 2. Itâ€™s Not Just Size That Matters - Small Language Models Are Also Few-Shot Learners (NAACL 2021) [[pdf](./Paper/Prompt/Not_Just_Size/It%E2%80%99s%20Not%20Just%20Size%20That%20Matters%20-%20Small%20Language%20Models%20Are%20Also%20Few-Shot%20Learners%20-%20NAACL%202021.pdf)]

`hard prompt` `MLMå‚æ•°æ›´æ–°`
    
> ### Motivation
> - PETã€iPETæ¡†æ¶åªèƒ½å¤„ç†å•ä¸ª[mask]çš„å®Œå½¢å¡«ç©ºï¼Œè€Œå®é™…ä»»åŠ¡å¯èƒ½ä¼šé‡åˆ°å¤šä¸ª[mask]ã€‚
> - æœ¬æ–‡ç›¸å½“äºPETã€iPETçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå°†å•[mask] tokenæ‹“å±•åˆ°kä¸ª[mask] tokensã€‚

<!-- <details>
<summary>Solution</summary>

> ![Framework]()

</details> -->

### 3. AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts (PrePrint 2020) [[pdf](./Paper/Prompt/AutoPrompt/Autoprompt%20-%20Eliciting%20knowledge%20from%20language%20models%20with%20automatically%20generated%20prompts%20-%20preprint.pdf)]

`soft prompt` `MLMå‚æ•°å›ºå®šï¼Ÿ`

> ### Motivation
> - åœ¨ä½¿ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œå°†ä¸‹æ¸¸ä»»åŠ¡å®šä¹‰ä¸ºå¡«ç©ºé¢˜å½¢å¼ï¼ˆä»»åŠ¡->è¯­è¨€æ¨¡å‹ï¼Œä¸è¯­è¨€æ¨¡å‹å®Œç¾å¥‘åˆï¼‰ï¼Œä½¿ç”¨promptæ²Ÿé€šé¢„è®­ç»ƒæ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡ï¼Œæœ‰åŠ©äºè¯„ä»·é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„çŸ¥è¯†æƒ…å†µï¼Œåˆ‡æ— éœ€å¼•å…¥å¤§é‡çš„æ–°å‚æ•°ã€‚
> - ä»¥å¾€ä¸“å®¶è®¾è®¡çš„promptï¼ˆhard promptï¼‰æˆæœ¬é«˜ï¼Œå¯¹è®¸å¤šä»»åŠ¡è€Œè¨€ä¸ç›´è§‚ï¼Œæ­¤å¤–ï¼Œæ¨¡å‹å¯¹promptåŒ…å«çš„contextä¿¡æ¯é«˜åº¦æ•æ„Ÿï¼Œå› æ­¤äººå·¥è®¾è®¡çš„promptå®¹æ˜“å¼•å…¥biasã€‚
> - ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºè‡ªåŠ¨å­¦ä¹ promptå’Œlabelçš„æ–¹å¼ã€‚

<details>
<summary>Solution</summary>

> ![Framework of AutoPrompt](./Paper/Prompt/AutoPrompt/fig1.png)

</details>

### 4. Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021) [[pdf](./Paper/Prompt/Prefix/Prefix-Tuning%20-%20Optimizing%20Continuous%20Prompts%20for%20Generation.pdf)][[GitHub](https://github.com/XiangLi1999/PrefixTuning)]

`soft prefix prompt tokens` `hard input sample prompt tokens` `MLMå‚æ•°å›ºå®š`

> ### Motivation
> - ä½¿ç”¨pre-train -> fine-tuneèŒƒå¼å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åº”ç”¨ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œé€šå¸¸éœ€è¦æ›´æ–°é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚å¯¹ä¸åŒä»»åŠ¡éœ€è¦åˆ†åˆ«fine-tuneä¸€æ¬¡æ¨¡å‹ï¼Œå¹¶å­˜å‚¨å…¶å‚æ•°ï¼Œè®­ç»ƒå’Œå­˜å‚¨å¼€é”€æ˜‚è´µã€‚
> - æœ¬æ–‡æå‡ºä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒè¿ç»­çš„prefix prompt tokensï¼ˆprefix of input samplesï¼‰ï¼Œè¶‹åŠ¿è¯­è¨€æ¨¡å‹æ‰§è¡Œä¸åŒä»»åŠ¡ã€‚

<details>
<summary>Solution</summary>

> ![Framework of Prefix-Tuning](./Paper/Prompt/Prefix/fig1.png)

</details>

### 5. PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains (2021) [[pdf](./Paper/Prompt/PADA/PADA%20-%20A%20Prompt-based%20Autoregressive%20Approach%20for%20Adaptation%20to%20Unseen%20Domains%20-%202021.pdf)]

`soft prompt` `MLM`

> ### Motivation

### 6. WARP: Word-level Adversarial ReProgramming (ACL 2021) [[pdf](./Paper/Prompt/Warp/WARP%20-%20Word-level%20Adversarial%20ReProgramming%20-%20ACL%20-%202021.pdf)]

`soft prompt` `MLMå‚æ•°å›ºå®šï¼Œåªæ›´æ–°æ’å…¥çš„promptå’Œlabel`

> ### Motivation

<details>
<summary>Solution</summary>

> ![Framework of Warp](./Paper/Prompt/Warp/fig1.png)

</details>

### 7. Making Pre-trained Language Models Better Few-shot Learners (2021) [[pdf](./Paper/Prompt/Few-shot_Learner/Making%20Pre-trained%20Language%20Models%20Better%20Few-shot%20Learners.pdf)]

<!-- `soft prompt` -->

> ### Motivation

<details>
<summary>Solution</summary>

> ![Prompt](./Paper/Prompt/Few-shot_Learner/fig2.png)

</details>

### 8. What Makes Good In-Context Examples for GPT-3? (2021) [[pdf](./Paper/Prompt/What_Makes_Good/What%20Makes%20Good%20In-Context%20Examples%20for%20GPT-3.pdf)]

<!-- `soft prompt` -->

> ### Motivation

<details>
<summary>Solution</summary>

> ![Prompt](./Paper/Prompt/What_Makes_Good/fig1.png)

</details>

# Prompt @ Recommendation

### 1. Prompt Learning for News Recommendation (SIGIR 2023) [[pdf](./Paper/Recommendation/NewsRec/Prompt%20Learning%20for%20News%20Recommendation.pdf)] ğŸ‘€

`soft prompt` `hard prompt` `hybrid prompt`

> ### Motivation
> - ä¼ ç»Ÿæ–°é—»æ¨èæ¨¡å‹ï¼šnews encoderã€user encoderã€similarity measure
> æœ€è¿‘å·¥ä½œå¼•å…¥é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥pre-trian -> fine-tuneèŒƒå¼ï¼Œç¼–ç æ–°é—»å†…å®¹ï¼Œä»¥è¿›è¡Œæ–°é—»æ¨èã€‚ä¸è¶³ï¼šå› ä¸ºæ–°é—»æ¨èä»»åŠ¡å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»»åŠ¡å­˜åœ¨gapï¼Œå½“å‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ã€‚
> - prompt learningä»¥pre-train -> prompt -> predictèŒƒå¼å°†ä¸‹æ¸¸ä»»åŠ¡å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹é½ï¼Œåœ¨è¯¸å¤šNLPä»»åŠ¡è¡¨ç°å‡ºè‰²ã€‚å—æ­¤å¯å‘ï¼Œæœ¬æ–‡å°†prompt learningæ¨¡å¼å¼•å…¥æ–°é—»æ¨èã€‚

<details>
<summary>Solution</summary>

> ![Framework of Prompt4NR](./Paper/Recommendation/NewsRec/fig1.png)

> ![Propmts of Prompt4NR](./Paper/Recommendation/NewsRec/fig2.png)

</details>

### 2. Personalized Prompt for Sequential Recommendation (2023) [[pdf](./Paper/Recommendation/SeqRC/Personalized%20Prompt%20for%20Sequential%20Recommendation.pdf)] ğŸ‘€

> ### Motivation
> - éšç€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç«çˆ†å‡ºåœˆï¼Œé‰´äºå…¶å…¶å¼ºå¤§çš„çŸ¥è¯†å‚¨å¤‡ã€è¯­ä¹‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ï¼Œæœ€è¿‘ç›¸å…³å·¥ä½œå¼€å§‹å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¼•å…¥åºåˆ—æ¨èã€‚ç›¸å…³ç ”ç©¶å°†ç”¨æˆ·å†å²è¡Œä¸ºè§†ä¸ºtokensï¼Œä½œä¸ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¾“å…¥ï¼Œä»¥ç¼“è§£çœŸå®åœºæ™¯ä¸‹ç”¨æˆ·è¡Œä¸ºæ•°æ®æ´—æ¼±é—®é¢˜ã€‚
> - æœ¬æ–‡å°†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¼•å…¥åˆ°åºåˆ—æ¨èä»»åŠ¡ä»¥å¤„ç†few-shotå’Œzero-shoté—®é¢˜ï¼Œé‡‡ç”¨prompt learningèŒƒå¼ï¼Œå°†æ¨èä»»åŠ¡å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¯¹é½ï¼Œä»¥æ›´é«˜æ•ˆä»é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æå–çŸ¥è¯†ã€‚

> ### Challenge
> - å¦‚ä½•å°†NLPé¢†åŸŸçš„prompt tuningèŒƒå¼å¼•å…¥æ¨èä»»åŠ¡ï¼Ÿ
> - å¦‚ä½•ä¸ºæ¨èç³»ç»Ÿè®¾è®¡ä¸ªæ€§åŒ–promptsï¼ˆuser-orientedï¼‰ã€‚

<details>
<summary>Solution</summary>

> ![Example of prompt tuning in recommendation](./Paper/Recommendation/SeqRC/fig1.png)

> ![Framework of Prompt4SeqRec](./Paper/Recommendation/SeqRC/fig2.png)

</details>

### 3. Personalized Prompt Learning for Explainable Recommendation (2023) [[pdf](./Paper/Recommendation/ExplainableRec/Personalized%20Prompt%20Learning%20for%20Explainable%20Recommendation.pdf)]

> ### Motivation
> - 

### 4. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation (2023) [[pdf](./Paper/Recommendation/GPT4Rec/GPT4Rec%20-%20A%20Generative%20Framework%20for%20Personalized%20Recommendation%20and%20User%20Interests%20Interpretation.pdf)]

> ### Motivation
> - 

### 5. Large Language Models are Zero-Shot Rankers for Recommender Systems (2023) [[pdf](./Paper/Recommendation/LLM4Rec/Large%20Language%20Models%20are%20Zero-Shot%20Rankers%20for%20Recommender%20Systems.pdf)]

> ### Motivation
> - 

# Survey

### 1. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing (2021) [[pdf](./Paper/Prompt/Pre-train/Pre-train%2C%20Prompt%2C%20and%20Predict%20-%20A%20Systematic%20Survey%20of%20Prompting%20Methods%20in%20Natural%20Language%20Processing.pdf)]

<!-- `soft prompt` -->

> ### Motivation
